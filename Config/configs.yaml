defaults:

    # Train Script
    logdir: /dev/null
    seed: 0
    task: dmc_walker_walk
    envs: 1
    envs_parallel: none
    render_size: [64, 64]
    dmc_camera: -1
    atari_grayscale: True
    time_limit: 0
    action_repeat: 1
    steps: 1e8
    report_every: 1e4
    ckpt_every: 0
    eval_every: 1e5
    eval_eps: 1
    prefill: 10000
    pretrain: 1
    train_every: 5
    train_steps: 1
    expl_until: 0
    replay: {capacity: 2e6, ongoing: False, minlen: 50, maxlen: 50, prioritize_ends: True, sample_recent: False, recent_episode_threshold: 0, initial_buffer_path: '', initial_buffer_capacity: 1e6, shuffle_blocks: False}
    dataset: {batch: 16, length: 50}
    goal_dataset: {batch: 150, length: 50}
    log_keys_video: ['image']
    log_keys_sum: '^$'
    log_keys_mean: '^$'
    log_keys_max: '^$'
    precision: 16
    jit: True
    state_key: none
    goal_key: none
    image_input: True
    no_render: False
    slurm_preempt: False
    gcp_train_factor: 1

    # Agent
    clip_rewards: tanh
    expl_behavior: greedy
    expl_noise: 0.0
    epsilon_expl_noise: 0.0
    eval_noise: 0.0
    eval_state_mean: False

    # World Model
    grad_heads: [decoder, reward, discount]
    pred_discount: True
    pred_reward: True
    rssm: {ensemble: 1, hidden: 1024, deter: 1024, stoch: 32, discrete: 32, act: elu, norm: none, std_act: sigmoid2, min_std: 0.1}
    encoder: {mlp_keys: '.*', cnn_keys: '.*', act: elu, norm: none, cnn_depth: 48, cnn_kernels: [4, 4, 4, 4], mlp_layers: [400, 400, 400, 400]}
    decoder: {mlp_keys: '.*', cnn_keys: '.*', act: elu, norm: none, cnn_depth: 48, cnn_kernels: [5, 5, 6, 6], mlp_layers: [400, 400, 400, 400]}
    reward_head: {layers: 4, units: 400, act: elu, norm: none, dist: mse}
    discount_head: {layers: 4, units: 400, act: elu, norm: none, dist: binary}
    loss_scales: {kl: 1.0, reward: 1.0, discount: 1.0, proprio: 1.0}
    kl: {free: 0.0, forward: False, balance: 0.8, free_avg: True}
    model_opt: {opt: adam, lr: 1e-4, eps: 1e-5, clip: 100, wd: 1e-6}

    # Actor Critic
    actor: {layers: 4, units: 400, act: elu, norm: none, dist: auto, min_std: 0.1}
    critic: {layers: 4, units: 400, act: elu, norm: none, dist: mse}
    actor_opt: {opt: adam, lr: 8e-5, eps: 1e-5, clip: 100, wd: 1e-6}
    critic_opt: {opt: adam, lr: 2e-4, eps: 1e-5, clip: 100, wd: 1e-6}
    discount: 0.99
    p2e_discount: 0.99
    discount_lambda: 0.95
    imag_horizon: 15
    actor_grad: auto
    actor_grad_mix: 0.1
    actor_ent: 2e-3
    slow_target: True
    slow_target_update: 100
    slow_target_fraction: 1
    slow_baseline: True
    reward_norm: {momentum: 1.0, scale: 1.0, eps: 1e-8}

    # Goal Conditioning Stuff
    gc_input: 'embed'
    gc_reward: 'dynamical_distance'
    gc_reward_shape: 'sum' # sum_diff
    pred_embed: True
    embed_head: {layers: 3, units: 400, act: elu, norm: none, dist: mse}
    training_goals: 'batch'
    train_env_goal_percent: 0.0
    labelled_env_multiplexing: False
    subgoal_threshold: 5.0
    goal_strategy: 'APS'
    gcp_rollout_every: 1 # run gcp rollout every X algo update.
    exp_rollout_every: 1 # run exp rollout every X algo update.
    two_policy_rollout_every: 1 # run two policy rollout every X algo update
    goal_update_every: 0 # how often to update goal picker
    go_expl_rand_ac: False
    planner: {
        batch: 500,
        cem_elite_ratio: 0.2,
        cost_use_p2e_value: True,
        evaluate_only: False,
        final_step_cost: True,
        goal_min: [0.0],
        goal_max: [0.0],
        horizon: 50,
        init_candidates: [123456789.0],
        init_env_goal_percent: 0.0,
        mega_prior: False,
        mppi_gamma: 10.0,
        optimization_steps: 5,
        planner_type: shooting_mppi,
        repeat_samples: 0,
        std_scale: 1.0,
        sample_replay: False,
        sample_env_goal_percent: 0.0,
    }

    # Dynamical Distance
    dd_inp : 'embed'
    dd_num_positives : 256
    dd_neg_sampling_factor : 0.0
    dd_norm_inp : False
    dd_norm_reg_label : True
    dd_train_imag : True
    dd_train_off_policy : False
    dd_distance : 'steps_to_go'
    dd_loss : 'regression'
    dd_prob_balance: 1.0
    dd_opt: {opt: adam, lr: 8e-5, eps: 1e-5, clip: 100, wd: 1e-6}

    # Exploration
    expl_intr_scale: 1.0
    expl_extr_scale: 0.0
    expl_opt: {opt: adam, lr: 3e-4, eps: 1e-5, clip: 100, wd: 1e-6}
    expl_head: {layers: 4, units: 400, act: elu, norm: none, dist: mse}
    expl_reward_norm: {momentum: 1.0, scale: 1.0, eps: 1e-8}
    disag_target: stoch
    disag_log: False
    disag_models: 10
    disag_offset: 1
    disag_action_cond: True
    expl_model_loss: kl


    # Our algorithm
    if_self_dd_net: False
    if_opt_embed_by_dd: False
    if_self_cluster: False
    cluster: {n_latent_landmarks: 10, learned_prior: True, LOG_STD_MIN: -20, LOG_STD_MAX: 2, lr_cluster: 3e-4, cluster_std_reg: 0.0, update_every: 1e3}
    if_assign_centroids: False
    centrods_assign_strategy: 'egc'
    centroids_assign_space: 'obs'
    if_assign_use_batch: False

    train_cluster_use_egc: False
    train_cluster_use_Normal: False

    if_multi_3_blcok_training_goal: False  # for three block stack

    if_reverse_action_converter: False
    reverse_action_converter: {layers: 3, units: 100, act: elu, norm: none, dist: auto, min_std: 0.1}
    if_one_step_predictor: False
    reverse_action_source: 'rac'
    if_reverse_episode_video_plot: False

    n_subgoals: 10
    early_stop: True
    
    # APS-Subtrans settings==================================================
    APS_subgoal_trans_rollout_every: 0 # run subgoal transition rollout every X algo updates
    APS_update_strategy: 2
    APS_total_time_limit: 300
    APS_goal_policy_time_limit: 100
    APS_if_explore: False
    APS_if_explore_time_limit: False
    APS_explore_time_limit: 20

    APS_if_subg_to_egc: False

    # egc-settings===========================================================
    if_egc_env_sample: True
    env_gcp_rollout_every: 1 # use env real goal to sample in real env
    EGC_if_explore: False
    EGC_goal_policy_time_limit: 100
    EGC_if_explore_time_limit: False
    EGC_explore_time_limit: 20

    if_env_random_reset: False

    if_actor_gs: False
    

APS_three_stack:
    action_repeat: 1
    actor_opt.lr: 8e-5
    actor_grad: 'auto'
    actor_ent: 0.00001
    clip_rewards: identity
    critic_opt.lr: 8e-5
    dataset: {batch: 45, length: 50}
    dd_neg_sampling_factor : 0.1
    decoder: {mlp_keys: 'observation', cnn_keys: '$^', mlp_layers: [400,400,400]}
    disag_models: 10
    disag_log: False
    encoder: {mlp_keys: 'observation', cnn_keys: '$^', mlp_layers: [400,400,400]}
    eval_every: 133    # 133 episodes = 2e4 steps.
    expl_behavior: 'Random'
    expl_extr_scale: 0.0
    exp_rollout_every: 0 # run exp rollout every X algo updates
    gc_reward: 'dynamical_distance'
    gc_input: 'embed'
    grad_heads: [decoder]
    goal_update_every: 50 # every 1 episode, update goal picker
    goal_key: 'goal'
    gcp_rollout_every: 0 # run gcp rollout every X algo updates
    kl.free: 1.0
    log_keys_max: 'is_success|log_subgoal_success'
    log_keys_video: ['none']
    model_opt.lr: 3e-4
    prefill: 25 # number of episodes of random exploration.
    precision: 16
    pretrain: 100
    pred_discount: False
    pred_reward: False
    pred_embed: True
    planner: {
        batch: 2000,
        cem_elite_ratio: 0.2,
        cost_use_p2e_value: True,
        evaluate_only: False,
        final_step_cost: True,
        goal_min: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
        goal_max: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0],
        horizon: 50,
        init_env_goal_percent: 0.0,
        repeat_samples: 0,
        std_scale: 0.5,
        sample_replay: False,
        sample_env_goal_percent: 0.0,
    }
    replay: {capacity: 1e7, ongoing: False, minlen: 50, maxlen: 50, prioritize_ends: False, sample_recent: True, recent_episode_threshold: 2000, shuffle_blocks: True } # 100K steps
    rssm: {ensemble: 1, hidden: 200, deter: 200, stoch: 50, discrete: 0, act: elu, norm: none, std_act: sigmoid2, min_std: 0.1}
    state_key: 'observation'
    steps: 3000000
   
    train_every: 5
    task: wallsdemofetchpnp3
    two_policy_rollout_every: 0 # run 2 policy rollout every X expl rollouts.

    # Our algorithm

    # goal_strategy: 'SubgoalPlanner'
    if_multi_3_blcok_training_goal: True  # for three block stack


    goal_strategy: 'APS' 
    n_subgoals: 20
    early_stop: True
    subgoal_threshold: 2.0
    time_limit: 150
    
    # APS-Subtrans settings==================================================
    APS_subgoal_trans_rollout_every: 1 # run subgoal transition rollout every X algo updates
    APS_update_strategy: 3
    APS_total_time_limit: 150
    APS_goal_policy_time_limit: 75
    APS_if_explore: False
    APS_if_explore_time_limit: False
    APS_explore_time_limit: 50
    APS_if_subg_to_egc: False

    # egc-settings===========================================================
    if_egc_env_sample: True
    env_gcp_rollout_every: 1 # use env real goal to sample in real env
    EGC_if_explore: False
    EGC_goal_policy_time_limit: 100
    EGC_if_explore_time_limit: False
    EGC_explore_time_limit: 20


APS_ant_maze:
    action_repeat: 1
    actor_opt.lr: 8e-5
    actor_grad: 'auto'
    actor_ent: 0.00001
    clip_rewards: identity
    critic_opt.lr: 8e-5
    dataset: {batch: 45, length: 50}
    dd_neg_sampling_factor : 0.1
    decoder: {mlp_keys: 'observation', cnn_keys: '$^', mlp_layers: [400,400,400]}
    disag_models: 10
    disag_log: False
    discount: 0.996
    encoder: {mlp_keys: 'observation', cnn_keys: '$^', mlp_layers: [400,400,400]}
    eval_every: 40 # 40 episodes = 2e4 steps.
    expl_behavior: 'Random'
    expl_extr_scale: 0.0
    exp_rollout_every: 0 # run exp rollout every X algo updates
    gc_reward: 'dynamical_distance'
    grad_heads: [decoder]
    goal_update_every: 50 # every 50 episodes, update goal picker
    goal_key: 'goal'
    gcp_rollout_every: 0 # run gcp rollout every X algo updates
    kl.free: 1.0
    log_keys_video: ['observation']
    model_opt.lr: 3e-4
    p2e_discount: 0.996
    prefill: 5
    precision: 16
    pretrain: 100
    pred_discount: False
    pred_reward: False
    pred_embed: True
    planner: {
        batch: 500,
        cem_elite_ratio: 0.2,
        cost_use_p2e_value: True,
        evaluate_only: False,
        final_step_cost: True,
        goal_min: [-0.75, -0.5, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0],
        goal_max: [5.0, 9.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0],
        horizon: 150,
        init_candidates: [0.0, 0.0, 8.19890515e-01,  9.95145602e-01,
            3.48547286e-02, -6.19350100e-02,  6.80766655e-02, -4.42372144e-02,
        -4.81461428e-02,  1.45511675e-02, -7.75746132e-02,  5.00618279e-02,
            3.65949561e-02,  4.99939194e-02,  1.02664477e-02, -2.27597298e-01,
            8.01758031e-02, -8.81610163e-02,  7.77121806e-02,  3.36722131e-02,
            2.27648027e-02, -2.24103019e-02, -3.77942221e-02, -6.56355237e-02,
            1.35722257e-01,  6.93039877e-02, -1.71162114e-01, -1.12083335e-01,
            1.76819156e-02],
        init_env_goal_percent: 0.0,
        mega_prior: False,
        mppi_gamma: 2.0,
        optimization_steps: 5,
        planner_type: shooting_mppi,
        repeat_samples: 0,
        std_scale: 10.0,
        sample_replay: False,
        sample_env_goal_percent: 0.0,
    }
    replay: {capacity: 2e6, ongoing: False, minlen: 50, maxlen: 50, prioritize_ends: False, sample_recent: True, recent_episode_threshold: 200 } # 100K steps
    rssm: {ensemble: 1, hidden: 200, deter: 200, stoch: 50, discrete: 0, act: elu, norm: none, std_act: sigmoid2, min_std: 0.1}
    state_key: 'observation'
    steps: 1000000
    train_every: 5
    task: hardumazefulldownscale
    two_policy_rollout_every: 0 # run 2 policy rollout every X expl rollouts.

    # Our algorithm
    goal_strategy: 'APS' 
    n_subgoals: 20
    early_stop: True
    subgoal_threshold: 2.0
    time_limit: 500
    
    # APS-Subtrans settings==================================================
    APS_subgoal_trans_rollout_every: 1 # run subgoal transition rollout every X algo updates
    APS_update_strategy: 2
    APS_total_time_limit: 500
    APS_goal_policy_time_limit: 250
    APS_if_explore: False
    APS_if_explore_time_limit: False
    APS_explore_time_limit: 50
    APS_if_subg_to_egc: False

    # egc-settings===========================================================
    if_egc_env_sample: True
    env_gcp_rollout_every: 1 # use env real goal to sample in real env
    EGC_if_explore: False
    EGC_goal_policy_time_limit: 100
    EGC_if_explore_time_limit: False
    EGC_explore_time_limit: 20


APS_point_maze:
    action_repeat: 1
    actor_opt.lr: 8e-5
    actor_grad: 'auto'
    actor_ent: 1e-4
    clip_rewards: identity
    critic_opt.lr: 8e-5
    dataset: {batch: 45, length: 50}
    dd_neg_sampling_factor : 0.0
    decoder: {mlp_keys: 'observation', cnn_keys: '$^', mlp_layers: [400,400,400]}
    disag_models: 10
    disag_log: False
    encoder: {mlp_keys: 'observation', cnn_keys: '$^', mlp_layers: [400,400,400]}
    eval_every: 200 # 200 episodes = 1e4 steps.
    expl_extr_scale: 0.0
    exp_rollout_every: 0 # run exp rollout every X algo updates
    gc_reward: 'dynamical_distance'
    grad_heads: [decoder]
    goal_update_every: 50 # every 50 episodes, update goal picker
    goal_key: 'goal'
    go_expl_rand_ac: False
    gcp_rollout_every: 0 # run gcp rollout every X algo updates
    kl.free: 1.0
    # log_keys_mean: 'metric_'
    # log_keys_sum: 'metric_|subgoal_dist'
    # log_keys_max: 'metric_|subgoal_success'
    model_opt.lr: 3e-4
    prefill: 50
    precision: 16
    pretrain: 100
    pred_discount: False
    pred_reward: False
    pred_embed: True
    planner: {planner_type: shooting_cem, horizon: 25, batch: 500, cem_elite_ratio: 0.2, optimization_steps: 5, std_scale: 4.0, init_candidates: [0.0, 0.0], goal_min: [0.0, 0.0],   goal_max: [10.0, 10.0], cost_use_p2e_value: True, final_step_cost: False }
    replay.prioritize_ends: False
    replay.sample_recent: True
    rssm: {ensemble: 1, hidden: 200, deter: 200, stoch: 50, discrete: 0, act: elu, norm: none, std_act: sigmoid2, min_std: 0.01}
    state_key: 'observation'
    steps: 1000000
    train_every: 5
    task: pointmaze
    two_policy_rollout_every: 0 # run 2 policy rollout every X algo updates

    goal_strategy: 'APS' 
    n_subgoals: 50
    early_stop: True
    subgoal_threshold: 1.0
    time_limit: 200

    expl_behavior: 'Random'
    
    # APS-Subtrans settings==================================================
    APS_subgoal_trans_rollout_every: 1 # run subgoal transition rollout every X algo updates
    APS_update_strategy: 2
    APS_total_time_limit: 100
    APS_goal_policy_time_limit: 20
    APS_if_explore: False
    APS_if_explore_time_limit: True
    APS_explore_time_limit: 50
    APS_if_subg_to_egc: False

    # egc-settings===========================================================
    if_egc_env_sample: True
    env_gcp_rollout_every: 1 # use env real goal to sample in real env
    EGC_if_explore: False
    EGC_goal_policy_time_limit: 100
    EGC_if_explore_time_limit: False
    EGC_explore_time_limit: 20


APS_walker:
    action_repeat: 2
    actor_opt.lr: 8e-5
    actor_ent: 0.00001
    clip_rewards: identity
    critic_opt.lr: 8e-5
    dataset: {batch: 45, length: 50}
    decoder: {mlp_keys: 'qpos', cnn_keys: '$^', mlp_layers: [400,400,400]}
    dd_neg_sampling_factor : 0.1
    disag_models: 10
    encoder: {mlp_keys: 'qpos', cnn_keys: '$^', mlp_layers: [400, 400, 400]}
    eval_every: 66 # 66 episodes = 1e4 steps.
    expl_behavior: 'Random'
    expl_extr_scale: 0.0
    exp_rollout_every: 0 # run exp rollout every X algo updates
    gc_reward: 'dynamical_distance'
    grad_heads: [decoder]
    goal_update_every: 50 # every 50 episodes, update goal picker
    goal_key: 'goal'
    gcp_rollout_every: 0 # run gcp rollout every X algo updates
    kl.free: 1.0
    # log_keys_mean: 'metric_'
    # log_keys_max: 'metric_'
    log_keys_video: ['none']
    model_opt.lr: 3e-4
    p2e_discount: 0.996
    prefill: 33 # 2500 steps.
    pretrain: 100
    pred_discount: False
    pred_reward: False
    pred_embed: True
    planner: {
        batch: 500,
        horizon: 75,
        mppi_gamma: 2.0,
        planner_type: shooting_mppi,
        goal_min: [-1.3, -20.0, -3.14, -3.14, -3.14,-3.14,-3.14,-3.14,-3.14,],
        goal_max: [0.5, 20.0, 3.14, 3.14, 3.14, 3.14, 3.14, 3.14, 3.14],
        std_scale: 5.0,
    }
    replay: {capacity: 2e6, ongoing: False, minlen: 50, maxlen: 50, prioritize_ends: False, sample_recent: True, recent_episode_threshold: 666 } # 100K steps
    rssm: {ensemble: 1, hidden: 200, deter: 200, stoch: 50, discrete: 0, act: elu, norm: none, std_act: sigmoid2, min_std: 0.1}
    state_key: 'qpos'
    steps: 1000000
    train_every: 5
    task: dmc_walker_walk_proprio
    two_policy_rollout_every: 0 # run 2 policy rollout every X expl rollouts.

    goal_strategy: 'APS' 
    n_subgoals: 20
    early_stop: True
    subgoal_threshold: 2.0
    time_limit: 150
    
    # APS-Subtrans settings==================================================
    APS_subgoal_trans_rollout_every: 1 # run subgoal transition rollout every X algo updates
    APS_update_strategy: 3
    APS_total_time_limit: 150
    APS_goal_policy_time_limit: 75
    APS_if_explore: False
    APS_if_explore_time_limit: False
    APS_explore_time_limit: 50
    APS_if_subg_to_egc: False

    # egc-settings===========================================================
    if_egc_env_sample: True
    env_gcp_rollout_every: 1 # use env real goal to sample in real env
    EGC_if_explore: False
    EGC_goal_policy_time_limit: 100
    EGC_if_explore_time_limit: False
    EGC_explore_time_limit: 20


APS_pick_place:
    action_repeat: 1
    actor_opt.lr: 8e-5
    actor_grad: 'auto'
    actor_ent: 0.00001
    clip_rewards: identity
    critic_opt.lr: 8e-5
    dataset: {batch: 45, length: 50}
    dd_neg_sampling_factor : 0.1
    decoder: {mlp_keys: 'observation', cnn_keys: '$^', mlp_layers: [400,400,400]}
    disag_models: 10
    disag_log: False
    encoder: {mlp_keys: 'observation', cnn_keys: '$^', mlp_layers: [400,400,400]}
    eval_every: 133    # 133 episodes = 2e4 steps.
    expl_extr_scale: 0.0
    exp_rollout_every: 0 # run exp rollout every X algo updates
    gc_reward: 'dynamical_distance'
    gc_input: 'state'
    grad_heads: [decoder]
    goal_update_every: 50 # every 1 episode, update goal picker
    goal_key: 'goal'
    gcp_rollout_every: 0 # run gcp rollout every X algo updates
    kl.free: 1.0
    log_keys_max: 'is_success|log_subgoal_success'
    log_keys_video: ['none']
    model_opt.lr: 3e-4
    prefill: 25 # number of episodes of random exploration.
    precision: 16
    pretrain: 100
    pred_discount: False
    pred_reward: False
    pred_embed: True
    planner: {
        batch: 2000,
        cem_elite_ratio: 0.2,
        cost_use_p2e_value: True,
        evaluate_only: False,
        final_step_cost: True,
        goal_min: [-inf, -inf, -inf],
        goal_max: [inf, inf, inf],
        horizon: 50,
        init_env_goal_percent: 0.0,
        repeat_samples: 0,
        std_scale: 0.5,
        sample_replay: False,
        sample_env_goal_percent: 0.0,
    }
    replay: {capacity: 1e7, ongoing: False, minlen: 50, maxlen: 50, prioritize_ends: False, sample_recent: True, recent_episode_threshold: 2000, shuffle_blocks: False } # 100K steps
    rssm: {ensemble: 1, hidden: 200, deter: 200, stoch: 50, discrete: 0, act: elu, norm: none, std_act: sigmoid2, min_std: 0.1}
    state_key: 'observation'
    steps: 1500000
    train_every: 5
    task: "PickAndPlace"
    two_policy_rollout_every: 0 # run 2 policy rollout every X expl rollouts.

    # Our algorithm

    disag_target: 'obs_decoded_gs'

    if_env_random_reset: True
    eval_state_mean: True
    expl_behavior: 'Random'

    goal_strategy: 'APS' 
    n_subgoals: 20
    early_stop: True
    subgoal_threshold: 2.0
    time_limit: 150
    
    # APS-Subtrans settings==================================================
    APS_subgoal_trans_rollout_every: 1 # run subgoal transition rollout every X algo updates
    APS_update_strategy: 0
    APS_total_time_limit: 150
    APS_goal_policy_time_limit: 75
    APS_if_explore: False
    APS_if_explore_time_limit: True
    APS_explore_time_limit: 50
    APS_if_subg_to_egc: False

    # egc-settings===========================================================
    if_egc_env_sample: True
    env_gcp_rollout_every: 1 # use env real goal to sample in real env
    EGC_if_explore: False
    EGC_goal_policy_time_limit: 100
    EGC_if_explore_time_limit: False
    EGC_explore_time_limit: 20


APS_rotate_block:
    action_repeat: 1
    actor_opt.lr: 8e-5
    actor_grad: 'auto'
    actor_ent: 0.00001
    clip_rewards: identity
    critic_opt.lr: 8e-5
    dataset: {batch: 45, length: 50}
    dd_neg_sampling_factor : 0.1
    decoder: {mlp_keys: 'observation', cnn_keys: '$^', mlp_layers: [400,400,400]}
    disag_models: 10
    disag_log: False
    encoder: {mlp_keys: 'observation', cnn_keys: '$^', mlp_layers: [400,400,400]}
    eval_every: 133    # 133 episodes = 2e4 steps.
    expl_extr_scale: 0.0
    exp_rollout_every: 0 # run exp rollout every X algo updates
    gc_reward: 'dynamical_distance'
    gc_input: 'state'
    grad_heads: [decoder]
    goal_update_every: 50 # every 1 episode, update goal picker
    goal_key: 'goal'
    gcp_rollout_every: 0 # run gcp rollout every X algo updates
    kl.free: 1.0
    log_keys_max: 'is_success|log_subgoal_success'
    log_keys_video: ['none']
    model_opt.lr: 3e-4
    prefill: 25 # number of episodes of random exploration.
    precision: 16
    pretrain: 100
    pred_discount: False
    pred_reward: False
    pred_embed: True
    planner: {
        batch: 2000,
        cem_elite_ratio: 0.2,
        cost_use_p2e_value: True,
        evaluate_only: False,
        final_step_cost: True,
        goal_min: [-inf, -inf, -inf],
        goal_max: [inf, inf, inf],
        horizon: 50,
        init_env_goal_percent: 0.0,
        repeat_samples: 0,
        std_scale: 0.5,
        sample_replay: False,
        sample_env_goal_percent: 0.0,
    }
    replay: {capacity: 1e7, ongoing: False, minlen: 50, maxlen: 50, prioritize_ends: False, sample_recent: True, recent_episode_threshold: 2000, shuffle_blocks: False } # 100K steps
    rssm: {ensemble: 1, hidden: 200, deter: 200, stoch: 50, discrete: 0, act: elu, norm: none, std_act: sigmoid2, min_std: 0.1}
    state_key: 'observation'
    steps: 3000000
    train_every: 5
    task: 'HandManipulateBlockRotateXYZ-v1'
    two_policy_rollout_every: 0 # run 2 policy rollout every X expl rollouts.

    # Our algorithm

    disag_target: 'obs_decoded_gs'
    expl_behavior: 'Random'

    if_env_random_reset: True
    eval_state_mean: True

    goal_strategy: 'APS' 
    n_subgoals: 20
    early_stop: True
    subgoal_threshold: 2.0
    time_limit: 150
    
    # APS-Subtrans settings==================================================
    APS_subgoal_trans_rollout_every: 1 # run subgoal transition rollout every X algo updates
    APS_update_strategy: 3
    APS_total_time_limit: 150
    APS_goal_policy_time_limit: 75
    APS_if_explore: False
    APS_if_explore_time_limit: True
    APS_explore_time_limit: 50
    APS_if_subg_to_egc: False

    # egc-settings===========================================================
    if_egc_env_sample: True
    env_gcp_rollout_every: 1 # use env real goal to sample in real env
    EGC_if_explore: False
    EGC_goal_policy_time_limit: 100
    EGC_if_explore_time_limit: False
    EGC_explore_time_limit: 20


APS_rotate_pen:
    action_repeat: 1
    actor_opt.lr: 8e-5
    actor_grad: 'auto'
    actor_ent: 0.00001
    clip_rewards: identity
    critic_opt.lr: 8e-5
    dataset: {batch: 45, length: 50}
    dd_neg_sampling_factor : 0.1
    decoder: {mlp_keys: 'observation', cnn_keys: '$^', mlp_layers: [400,400,400]}
    disag_models: 10
    disag_log: False
    encoder: {mlp_keys: 'observation', cnn_keys: '$^', mlp_layers: [400,400,400]}
    eval_every: 133    # 133 episodes = 2e4 steps.
    expl_extr_scale: 0.0
    exp_rollout_every: 0 # run exp rollout every X algo updates
    gc_reward: 'dynamical_distance'
    gc_input: 'state'
    grad_heads: [decoder]
    goal_update_every: 50 # every 1 episode, update goal picker
    goal_key: 'goal'
    gcp_rollout_every: 0 # run gcp rollout every X algo updates
    kl.free: 1.0
    log_keys_max: 'is_success|log_subgoal_success'
    log_keys_video: ['none']
    model_opt.lr: 3e-4
    prefill: 25 # number of episodes of random exploration.
    precision: 16
    pretrain: 100
    pred_discount: False
    pred_reward: False
    pred_embed: True
    planner: {
        batch: 2000,
        cem_elite_ratio: 0.2,
        cost_use_p2e_value: True,
        evaluate_only: False,
        final_step_cost: True,
        goal_min: [-inf, -inf, -inf],
        goal_max: [inf, inf, inf],
        horizon: 50,
        init_env_goal_percent: 0.0,
        repeat_samples: 0,
        std_scale: 0.5,
        sample_replay: False,
        sample_env_goal_percent: 0.0,
    }
    replay: {capacity: 1e7, ongoing: False, minlen: 50, maxlen: 50, prioritize_ends: False, sample_recent: True, recent_episode_threshold: 2000, shuffle_blocks: False } # 100K steps
    rssm: {ensemble: 1, hidden: 200, deter: 200, stoch: 50, discrete: 0, act: elu, norm: none, std_act: sigmoid2, min_std: 0.1}
    state_key: 'observation'
    steps: 2000000
    train_every: 5
    task: 'HandManipulatePenRotate-v1'
    two_policy_rollout_every: 0 # run 2 policy rollout every X expl rollouts.

    # Our algorithm

    disag_target: 'obs_decoded_gs'
    expl_behavior: 'Random'

    if_env_random_reset: True
    eval_state_mean: True

    goal_strategy: 'APS' 
    n_subgoals: 20
    early_stop: True
    subgoal_threshold: 2.0
    time_limit: 150
    
    # APS-Subtrans settings==================================================
    APS_subgoal_trans_rollout_every: 1 # run subgoal transition rollout every X algo updates
    APS_update_strategy: 1
    APS_total_time_limit: 150
    APS_goal_policy_time_limit: 75
    APS_if_explore: False
    APS_if_explore_time_limit: True
    APS_explore_time_limit: 50
    APS_if_subg_to_egc: False

    # egc-settings===========================================================
    if_egc_env_sample: True
    env_gcp_rollout_every: 1 # use env real goal to sample in real env
    EGC_if_explore: False
    EGC_goal_policy_time_limit: 100
    EGC_if_explore_time_limit: False
    EGC_explore_time_limit: 20


APS_peg_insert:
    action_repeat: 1
    actor_opt.lr: 8e-5
    actor_grad: 'auto'
    actor_ent: 0.00001
    clip_rewards: identity
    critic_opt.lr: 8e-5
    dataset: {batch: 45, length: 50}
    dd_neg_sampling_factor : 0.1
    decoder: {mlp_keys: 'observation', cnn_keys: '$^', mlp_layers: [400,400,400]}
    disag_models: 10
    disag_log: False
    encoder: {mlp_keys: 'observation', cnn_keys: '$^', mlp_layers: [400,400,400]}
    eval_every: 133    # 133 episodes = 2e4 steps.
    expl_behavior: 'Random'
    expl_extr_scale: 0.0
    exp_rollout_every: 0 # run exp rollout every X algo updates
    gc_reward: 'dynamical_distance'
    gc_input: 'state'
    grad_heads: [decoder]
    goal_update_every: 50 # every 1 episode, update goal picker
    goal_key: 'goal'
    gcp_rollout_every: 0 # run gcp rollout every X algo updates
    kl.free: 1.0
    log_keys_max: 'is_success|log_subgoal_success'
    log_keys_video: ['none']
    model_opt.lr: 3e-4
    prefill: 25 # number of episodes of random exploration.
    precision: 16
    pretrain: 100
    pred_discount: False
    pred_reward: False
    pred_embed: True
    planner: {
        batch: 2000,
        cem_elite_ratio: 0.2,
        cost_use_p2e_value: True,
        evaluate_only: False,
        final_step_cost: True,
        goal_min: [-inf, -inf, -inf, -inf, -inf, -inf, -inf],
        goal_max: [inf, inf, inf, inf, inf, inf, inf],
        horizon: 50,
        init_env_goal_percent: 0.0,
        repeat_samples: 0,
        std_scale: 0.5,
        sample_replay: False,
        sample_env_goal_percent: 0.0,
    }
    replay: {capacity: 1e7, ongoing: False, minlen: 50, maxlen: 50, prioritize_ends: False, sample_recent: True, recent_episode_threshold: 2000, shuffle_blocks: False } # 100K steps
    rssm: {ensemble: 1, hidden: 200, deter: 200, stoch: 50, discrete: 0, act: elu, norm: none, std_act: sigmoid2, min_std: 0.1}
    state_key: 'observation'
    steps: 1500000
    train_every: 5
    task: 'PegInsertionSide-v0'
    two_policy_rollout_every: 0 # run 2 policy rollout every X expl rollouts.

    # Our algorithm

    disag_target: 'obs_decoded_gs'

    if_env_random_reset: True
    eval_state_mean: True

    goal_strategy: 'APS' 
    n_subgoals: 20
    early_stop: True
    subgoal_threshold: 2.0
    time_limit: 150
    
    # APS-Subtrans settings==================================================
    APS_subgoal_trans_rollout_every: 1 # run subgoal transition rollout every X algo updates
    APS_update_strategy: 0
    APS_total_time_limit: 250
    APS_goal_policy_time_limit: 75
    APS_if_explore: True
    APS_if_explore_time_limit: True
    APS_explore_time_limit: 50
    APS_if_subg_to_egc: False

    # egc-settings===========================================================
    if_egc_env_sample: True
    env_gcp_rollout_every: 1 # use env real goal to sample in real env
    EGC_if_explore: False
    EGC_goal_policy_time_limit: 100
    EGC_if_explore_time_limit: False
    EGC_explore_time_limit: 20


APS_fetchslide:
    action_repeat: 1
    actor_opt.lr: 8e-5
    actor_grad: 'auto'
    actor_ent: 0.00001
    clip_rewards: identity
    critic_opt.lr: 8e-5
    dataset: {batch: 45, length: 50}
    dd_neg_sampling_factor : 0.1
    decoder: {mlp_keys: 'observation', cnn_keys: '$^', mlp_layers: [400,400,400]}
    disag_models: 10
    disag_log: False
    encoder: {mlp_keys: 'observation', cnn_keys: '$^', mlp_layers: [400,400,400]}
    eval_every: 133    # 133 episodes = 2e4 steps.
    expl_extr_scale: 0.0
    exp_rollout_every: 0 # run exp rollout every X algo updates
    gc_reward: 'dynamical_distance'
    gc_input: 'state'
    grad_heads: [decoder]
    goal_update_every: 50 # every 1 episode, update goal picker
    goal_key: 'goal'
    gcp_rollout_every: 0 # run gcp rollout every X algo updates
    kl.free: 1.0
    log_keys_max: 'is_success|log_subgoal_success'
    log_keys_video: ['none']
    model_opt.lr: 3e-4
    prefill: 25 # number of episodes of random exploration.
    precision: 16
    pretrain: 100
    pred_discount: False
    pred_reward: False
    pred_embed: True
    planner: {
        batch: 2000,
        cem_elite_ratio: 0.2,
        cost_use_p2e_value: True,
        evaluate_only: False,
        final_step_cost: True,
        goal_min: [-inf, -inf, -inf],
        goal_max: [inf, inf, inf],
        horizon: 50,
        init_env_goal_percent: 0.0,
        repeat_samples: 0,
        std_scale: 0.5,
        sample_replay: False,
        sample_env_goal_percent: 0.0,
    }
    replay: {capacity: 1e7, ongoing: False, minlen: 50, maxlen: 50, prioritize_ends: False, sample_recent: True, recent_episode_threshold: 2000, shuffle_blocks: False } # 100K steps
    rssm: {ensemble: 1, hidden: 200, deter: 200, stoch: 50, discrete: 0, act: elu, norm: none, std_act: sigmoid2, min_std: 0.1}
    state_key: 'observation'
    steps: 1500000
    train_every: 5
    task: "FetchSlide-v2"
    two_policy_rollout_every: 0 # run 2 policy rollout every X expl rollouts.

    # Our algorithm

    disag_target: 'obs_decoded_gs'

    if_env_random_reset: True
    eval_state_mean: True
    expl_behavior: 'Random'

    goal_strategy: 'APS' 
    n_subgoals: 20
    early_stop: True
    subgoal_threshold: 2.0
    time_limit: 150
    
    # APS-Subtrans settings==================================================
    APS_subgoal_trans_rollout_every: 1 # run subgoal transition rollout every X algo updates
    APS_update_strategy: 0
    APS_total_time_limit: 150
    APS_goal_policy_time_limit: 75
    APS_if_explore: False
    APS_if_explore_time_limit: True
    APS_explore_time_limit: 50
    APS_if_subg_to_egc: False

    # egc-settings===========================================================
    if_egc_env_sample: True
    env_gcp_rollout_every: 1 # use env real goal to sample in real env
    EGC_if_explore: False
    EGC_goal_policy_time_limit: 100
    EGC_if_explore_time_limit: False
    EGC_explore_time_limit: 20
